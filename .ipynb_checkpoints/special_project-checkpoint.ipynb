{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2019:][iapr2019] Special project\n",
    "\n",
    "**Group members:**\n",
    "    1- first name and last name,\n",
    "    2- first name and last name,\n",
    "    3- first name and last name\n",
    "\n",
    "**Due date:** 30.05.2019\n",
    "\n",
    "[iapr2019]: https://github.com/LTS5/iapr-2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Please find the description of this special project via [this link].\n",
    "\n",
    "[this link]: https://github.com/LTS5/iapr-2019/blob/master/project/special_project_description.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Finding varroas by segmentation\n",
    "Add your implementation for ''**detect_by_segmentation**'' function. Please make sure the input and output follows the mentioned format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import skimage.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_train.pickle', 'rb') as handle:\n",
    "    df_train = pickle.load(handle)\n",
    "\n",
    "with open('df_test.pickle', 'rb') as handle:\n",
    "    df_test = pickle.load(handle)\n",
    "    \n",
    "with open('df_val.pickle', 'rb') as handle:\n",
    "    df_val = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59023fbd579e52581ddede9f_32.00px_19</td>\n",
       "      <td>915.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59023fbd579e52581ddede9f_32.00px_19</td>\n",
       "      <td>161.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59023fbd579e52581ddede9f_32.00px_19</td>\n",
       "      <td>339.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59023fbd579e52581ddede9f_32.00px_19</td>\n",
       "      <td>727.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59023fbd579e52581ddede9f_32.00px_19</td>\n",
       "      <td>851.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>897.0</td>\n",
       "      <td>696.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename   xmin   ymin   xmax   ymax\n",
       "0  59023fbd579e52581ddede9f_32.00px_19  915.0  121.0  961.0  167.0\n",
       "1  59023fbd579e52581ddede9f_32.00px_19  161.0  619.0  206.0  665.0\n",
       "2  59023fbd579e52581ddede9f_32.00px_19  339.0  532.0  384.0  578.0\n",
       "3  59023fbd579e52581ddede9f_32.00px_19  727.0  743.0  773.0  789.0\n",
       "4  59023fbd579e52581ddede9f_32.00px_19  851.0  650.0  897.0  696.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_fct(DATA_PATH,IMAGE_PATH,PATH, df_data):\n",
    "    '''\n",
    "    Returns a list of images\n",
    "    '''\n",
    "    PATH = DATA_PATH+IMAGE_PATH+PATH\n",
    "    im_names = df_data.filename.unique()\n",
    "    filenames = [os.path.join(PATH, name) + '.jpg' for name in im_names]\n",
    "    images = skimage.io.imread_collection(filenames)\n",
    "    print('Number of images: ', len(images))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  532\n",
      "Number of images:  33\n",
      "Number of images:  103\n"
     ]
    }
   ],
   "source": [
    "# Load images\n",
    "DATA_PATH = \"../../project-data\"\n",
    "IMAGE_PATH = \"/images\"\n",
    "TRAIN_PATH = \"/train\"\n",
    "TEST_PATH = \"/test\"\n",
    "VALIDATION_PATH = \"/validation\"\n",
    "\n",
    "train_data = pd.read_pickle(\"df_train.pickle\")\n",
    "test_data = pd.read_pickle(\"df_test.pickle\")\n",
    "val_data = pd.read_pickle(\"df_val.pickle\")\n",
    "\n",
    "train_images = get_images_fct(DATA_PATH,IMAGE_PATH, TRAIN_PATH, train_data)\n",
    "test_images = get_images_fct(DATA_PATH,IMAGE_PATH, TEST_PATH, test_data)\n",
    "val_images = get_images_fct(DATA_PATH,IMAGE_PATH, VALIDATION_PATH, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import imutils\n",
    "\n",
    "def pyramid(image, scale, minSize=(100, 100)):\n",
    "\t# yield the original image\n",
    "\tyield image\n",
    "\n",
    "\t# keep looping over the pyramid\n",
    "\twhile True:\n",
    "\t\t# compute the new dimensions of the image and resize it\n",
    "\t\tw = int(image.shape[1] / scale)\n",
    "\t\timage = imutils.resize(image, width=w)\n",
    "\n",
    "\t\t# if the resized image does not meet the supplied minimum\n",
    "\t\t# size, then stop constructing the pyramid\n",
    "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t# yield the next image in the pyramid\n",
    "\t\tyield image\n",
    "\n",
    "def sliding_window(image, stepSize, windowSize):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All train_images have been processed\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Size of the sliding windows (a bit bigger than a varroa which is approximately a square of 32x32)\n",
    "(winW, winH) = (40, 40)\n",
    "\n",
    "#Train images processing\n",
    "for i in range(len(train_images)):\n",
    "    # loop over the sliding window for each layer of the pyramid\n",
    "    for (x, y, window) in sliding_window(train_images[i], stepSize=32, windowSize=(winW, winH)):\n",
    "        # if the window does not meet our desired window size, ignore it\n",
    "        if window.shape[0] != winH or window.shape[1] != winW:\n",
    "            continue\n",
    "\n",
    "print(\"All train_images have been processed\")\n",
    "\n",
    "\n",
    "\n",
    "        # THIS IS WHERE YOU WOULD PROCESS YOUR WINDOW, SUCH AS APPLYING A\n",
    "        # MACHINE LEARNING CLASSIFIER TO CLASSIFY THE CONTENTS OF THE\n",
    "        # WINDOW\n",
    "\n",
    "        # since we do not have a classifier, we'll just draw the window\n",
    "        #clone = train_images[1].copy()\n",
    "        #cv2.rectangle(clone, (x, y), (x + winW, y + winH), (0, 255, 0), 2)\n",
    "        #cv2.imshow(\"Window\", clone)\n",
    "        #cv2.waitKey(0)\n",
    "        #time.sleep(0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_segmentation(img):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "\n",
    "    #Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your implementation. Report the Precision, Recall and F1-score, by using all 50 images of the test-set, and considering 0.3 as the IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement your first detector\n",
    "\n",
    "Write your function(s) for the second part. Feel free to change the name of the function and add your additional functions, but please make sure their input and output follows the mentioned format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_by_method_1(img):\n",
    "    '''\n",
    "    Input: One single image\n",
    "    Output: A numpy array containing coordonates of all detected varroas, with the following format: \n",
    "            [[x_1, y_1, w_1, h_2], [x_2, y_2, w_1, h_2], ..., [x_n, y_n, w_n, h_n]] \n",
    "            where ''n'' is the number of detected varroas.\n",
    "    '''\n",
    "\n",
    "    #Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your implementation. Report the Precision, Recall and F1-score, by using all 50 images of the test-set, and considering 0.3 as the IoU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using MLP and CNNs\n",
    "\n",
    "Add your implementation for the thrid part. Feel free to add your desirable functions, but please make sure you have proper functions for the final detection, where their input and output follows the same format as the previous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You can generate a json submission file by using the function ''**generate_pred_json**''. This prediction file can be uploaded online for evaluation (Please refer to section 3 of the project description for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def generate_pred_json(data, tag='baseline'):\n",
    "    '''\n",
    "    Input\n",
    "    - data: Is a dictionary d, such that:\n",
    "          d = { \n",
    "              \"ID_1\": [], \n",
    "              \"ID_2\": [[x_21, y_21, w_21, h_21], [x_22, y_22, w_22, h_22]], \n",
    "              ... \n",
    "              \"ID_i\": [[x_i1, y_i1, w_i1, h_i1], ..., [x_iJ, y_iJ, w_iJ, h_iJ]],\n",
    "              ... \n",
    "              \"ID_N\": [[x_N1, y_N1, w_N1, h_N1]],\n",
    "          }\n",
    "          where ID is the string id of the image (e.i. 5a05e86fa07d56baef59b1cb_32.00px_1) and the value the Kx4 \n",
    "          array of intergers for the K predicted bounding boxes (e.g. [[170, 120, 15, 15]])\n",
    "    - tag: (optional) string that will be added to the name of the json file.\n",
    "    Output\n",
    "      Create a json file, \"prediction_[tag].json\", conatining the prediction to EvalAI format.\n",
    "    '''\n",
    "    unvalid_key = []\n",
    "    _data = data.copy()\n",
    "    for key, value in _data.items():\n",
    "        try:\n",
    "            # Try to convert to numpy array and cast as closest int\n",
    "            print(key)\n",
    "            v = np.around(np.array(value)).astype(int)\n",
    "            # Check is it is a 2d array with 4 columns (x,y,w,h)\n",
    "            if v.ndim != 2 or v.shape[1] != 4:\n",
    "                unvalid_key.append(key)\n",
    "            # Id must be a string\n",
    "            if not isinstance(key, str):\n",
    "                unvalid_key.append(key)\n",
    "            _data[key] = v.tolist()\n",
    "        # Deal with not consistant array size and empty predictions\n",
    "        except (ValueError, TypeError):\n",
    "            unvalid_key.append(key)\n",
    "    # Remove unvalid key from dictionnary\n",
    "    for key in unvalid_key: del _data[key]\n",
    "    \n",
    "    with open('prediction_{}.json'.format(tag), 'w') as outfile:\n",
    "        json.dump(_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
